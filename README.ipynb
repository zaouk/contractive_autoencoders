{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow implementation of a contractive Auto-Encoder [[1](https://icml.cc/Conferences/2011/papers/455_icmlpaper.pdf)]\n",
    "\n",
    "This is a personal attempt to reimplement a contractive autoencoder (with FC layers uniquely) as described in the original [paper](https://icml.cc/Conferences/2011/papers/455_icmlpaper.pdf) by Rifai et Al.\n",
    "\n",
    "To the best of our knowledge, this is the first implementation done with native Tensorflow.\n",
    "\n",
    "I also provide in this repository extensions to the original contractive loss, to include a contraction term I computed for (in addition to the one provided in the [paper](https://icml.cc/Conferences/2011/papers/455_icmlpaper.pdf) for 1 layer of sigmoid non linearity):\n",
    "* 2 layers of sigmoid non linearity\n",
    "* 1 layer of relu non linearity\n",
    "* 2 layers of relu non linearity\n",
    "\n",
    "\n",
    "The main files are:\n",
    "\n",
    "- `contractive.py` provides the implementation of the auto-encoder with a loss of 2 terms: (1) Reconstruction, (2) Contraction\n",
    "\n",
    "$$\\mathcal{J} = \\displaystyle\\sum_{x \\in D_n} \\left(L(x, g(f(x)) + \\lambda ||J_f(x)||^2_F \\right)$$\n",
    "\n",
    "        (f is the encoding function, g is the decoding function, L being the MSE in our case)\n",
    "    \n",
    "\n",
    "\n",
    "### Requirements:\n",
    "The code requires tensorflow >= 2.0\n",
    "\n",
    "### Implementation details and Jacobian calculations\n",
    "\n",
    "$||J_f(x)||^2_F = \\displaystyle\\sum_{i, j} \\left(\\displaystyle\\frac{\\partial h_i(x)}{\\partial x_j}\\right)^2$\n",
    "\n",
    "* $x$ is the input vector\n",
    "* $h$ is the encoding vector\n",
    "\n",
    "\n",
    "#### 1 layer of sigmoid activation:\n",
    "$h = f(x) = sigmoid(Wx + b)$\n",
    "\n",
    "with:\n",
    "* $x = [x_1, x_2, ..., x_{dx}]^T$\n",
    "* $h = [h_1, h_2, ..., h_{dh}]^T$\n",
    "* $b = [b_1, b_2, ..., b_{dh}]^T$\n",
    "* $d_h$: number of hidden dimensions of the encoding layer\n",
    "* $d_x$: number of input dimensions\n",
    "* W is of shape $[d_h, d_x]$\n",
    "\n",
    "\n",
    "$\\boxed{||J_f(x)||^2_F = \\displaystyle\\sum_{i=1}^{d_h} h_i^2 (1-h_i)^2 \\displaystyle\\sum_{j=1}^{d_x} w_{ij}^2}$\n",
    "\n",
    "\n",
    "#### 2 layers of sigmoid activations:\n",
    "$z(x) = sigmoid(W^{(1)}x + b^{(1)})$\n",
    "\n",
    "$h = f(x) = f(z(x)) = sigmoid(W^{(2)}z + b^{(2)})$\n",
    "\n",
    "with:\n",
    "* $x = [x_1, x_2, ..., x_{dx}]^T$\n",
    "* $z = [x_1, x_2, ..., x_{dz}]^T$\n",
    "* $h = [h_1, h_2, ..., h_{dh}]^T$\n",
    "* $b^{(1)} = [b^{(1)}_1, b^{(1)}_2, ..., b^{(1)}_{dz}]^T$\n",
    "* $b^{(2)} = [b^{(2)}_1, b^{(2)}_2, ..., b^{(2)}_{dh}]^T$\n",
    "\n",
    "* $d_h$: number of hidden dimensions of the encoding layer\n",
    "* $d_z$: number of hidden dimensions of the first layer\n",
    "* $d_x$: number of input dimensions\n",
    "* $W^{(1)}$ is of shape $[d_z, d_x]$\n",
    "* $W^{(2)}$ is of shape $[d_h, d_z]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\displaystyle\\frac{\\partial h_i(x)}{\\partial x_j} = ?$\n",
    "\n",
    "\n",
    "Applying the chain rule:\n",
    "$\\displaystyle\\frac{\\partial h_i(x)}{\\partial x_j} = \\displaystyle\\sum_k \\frac{\\partial h_i}{\\partial z_k} \\frac{\\partial z_k}{\\partial x_j}$ \n",
    "\n",
    "\n",
    "$\\displaystyle \\frac{\\partial h_i}{\\partial z_k} = h_i (1 - h_i) w^{(2)}_{ik}$\n",
    "\n",
    "$\\displaystyle \\frac{\\partial z_k}{\\partial x_j} = z_k (1 - z_k) w^{(1)}_{kj}$\n",
    "\n",
    "\n",
    "$\\displaystyle\\frac{\\partial h_i}{\\partial x_j} = h_i (1 - h_i) \\sum_k z_k (1 - z_k) w^{(1)}_{kj}w^{(2)}_{ik}$\n",
    "\n",
    "\n",
    "$||J_f(x)||^2_F = \\displaystyle\\sum_{i, j} \\left(\\displaystyle\\frac{\\partial h_i(x)}{\\partial x_j}\\right)^2$\n",
    "\n",
    "$\\boxed{||J_f(x)||^2_F = \\displaystyle\\sum_{i=1}^{dh} h_i^2 (1-h_i)^2 \\sum_{j=1}^{dx} \\left(\\sum_{k=1}^{dz} z_k (1 - z_k) w^{(1)}_{kj}w^{(2)}_{ik}\\right)^2}$\n",
    "\n",
    "\n",
    "\n",
    "#### 1 layer of ReLu activation:\n",
    "$u(x) = Wx + b$ (preactivation)\n",
    "\n",
    "$h = f(x) = relu(u(x)) = relu(Wx + b)$\n",
    "\n",
    "\n",
    "$\\displaystyle\\frac{\\partial h_i(x)}{\\partial x_j} = w_{ij} \\mathbb{1}_{u_i > 0} $ \n",
    "\n",
    "(I need to verify if the latter equation is correct when preactivation=0)\n",
    "\n",
    "\n",
    "$\\boxed{||J_f(x)||^2_F = \\displaystyle\\sum_{i=1}^{dh} \\mathbb{1}_{u_i > 0} \\sum_{j=1}^{dx} w_{ij}^2}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2 layers of ReLu activations:\n",
    "$z(x) = relu(W^{(1)}x + b^{(1)})$\n",
    "\n",
    "$h = f(x) = f(z(x)) = relu(W^{(2)}z + b^{(2)})$\n",
    "\n",
    "Let's denote by $\\tilde{h}$ the pre-activation of $h$ and $\\tilde{z}$ the pre-activation of $z$\n",
    "\n",
    "Applying again the chain rule:\n",
    "$\\displaystyle\\frac{\\partial h_i(x)}{\\partial x_j} = \\displaystyle\\sum_k \\frac{\\partial h_i}{\\partial z_k} \\frac{\\partial z_k}{\\partial x_j}$ \n",
    "\n",
    "\n",
    "$\\displaystyle \\frac{\\partial h_i}{\\partial z_k} =  w^{(2)}_{ik} \\mathbb{1}_{\\tilde{h}_i > 0} $\n",
    "\n",
    "$\\displaystyle \\frac{\\partial z_k}{\\partial x_j} =  w^{(1)}_{kj}  \\mathbb{1}_{\\tilde{z}_k > 0} $\n",
    "\n",
    "\n",
    "$\\displaystyle\\frac{\\partial h_i}{\\partial x_j} = \\mathbb{1}_{\\tilde{h}_i > 0} \\displaystyle\\sum_{k=1}^{dz} \\mathbb{1}_{\\tilde{z}_k > 0} w^{(1)}_{kj} w^{(2)}_{ik} $ \n",
    "\n",
    "\n",
    "$\\boxed{||J_f(x)||^2_F = \\displaystyle\\sum_{i=1}^{dh} \\mathbb{1}_{\\tilde{h}_i > 0} \\sum_{j=1}^{dx} \\left(\\sum_{k=1}^{dz} \\mathbb{1}_{\\tilde{z}_k > 0} w^{(1)}_{kj}w^{(2)}_{ik}\\right)^2}$\n",
    "\n",
    "\n",
    "### Description of contractive plus:\n",
    "`contractiveplus.py` provides a hybrid implementation of (1) a special auto-encoder from our prior [work](http://www.vldb.org/pvldb/vol12/p1934-zaouk.pdf) on modeling Spark Streaming workloads (2) the contractive auto-encoder. In this case, the loss function will have 3 terms: (1) Reconstruction term (2) Additional Input term (3) Contraction term\n",
    "\n",
    "$$\\mathcal{J} = \\displaystyle\\sum_{\\{x, \\theta\\} \\in D_n} \\left(L(x, g(f(x)) + \\gamma L(\\theta, f_v(x)) + \\lambda ||J_{f_{iv}}(x)||^2_F \\right)$$\n",
    "\n",
    "$f$ is the encoding function, $g$ is the decoding function, $L$ is the MSE in our case\n",
    "        \n",
    "Here we assume that the bottleneck layer $f(x)$ can be broken into two parts: \n",
    "* Invariant part: $f_{iv}(x)$ that's the desired encoding, and for which we add a Jacobian term\n",
    "* Variant part: $f_v(x)$ that should approximate the known parameters $\\theta$ that generated the observation $x$\n",
    "\n",
    "So in this case, $f(x) = \\left( f_{iv}(x) || f_{v}(x)\\right )$\n",
    "\n",
    "### Examples:\n",
    "TODO\n",
    "\n",
    "\n",
    "### References:\n",
    "[[1](https://icml.cc/Conferences/2011/papers/455_icmlpaper.pdf)] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-encoders: Explicit invariance during feature extraction. In Proceedings of\n",
    "the 28th International Conference on International Conference on Machine Learning,\n",
    "ICML’11, pages 833–840, USA, 2011. Omnipress.\n",
    "\n",
    "\n",
    "<hr>\n",
    "Last edied on April 17th, 2020 by K. Zaouk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
